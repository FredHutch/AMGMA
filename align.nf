#!/usr/bin/env nextflow

// Set default parameters
params.help = false
params.db = null
params.geneshot_folder = null
params.geneshot_results_hdf = null
params.geneshot_details_hdf = null
params.geneshot_fasta = null
params.output_folder = null
params.output_prefix = null
params.min_coverage = 50
params.min_identity = 80
params.top = 5
params.fdr_method = "fdr_bh"
params.alpha = 0.2
params.blast = false
params.no_associations = false
params.min_cag_size = 5
params.min_cag_prop = 0.25

// Commonly used containers
container__pandas = "quay.io/fhcrc-microbiome/python-pandas:v1.0.3_py38_h5py"
container_diamond = "quay.io/fhcrc-microbiome/docker-diamond:v0.9.31--3"
container__blast = "ncbi/blast:2.10.1"

// Function which prints help message text
def helpMessage() {
    log.info"""
Usage:

nextflow run FredHutch/AMGMA <ARGUMENTS>

Required Arguments:
--db                  AMGMA database(s) (ends with .tar), with commas delimiting multiple databases
--geneshot_hdf        Results HDF file output by GeneShot, containing CAG information
--geneshot_dmnd       DIAMOND database for the gene catalog generated by GeneShot
--output_folder       Folder to write output HDF file into
--output_hdf          Name of the output HDF file to write to the output folder

Optional Arguments:
--filter              Only analyze parameters which contain the specified substring
                      (Example: --filter BMI will only analyze the covariate matching "*BMI*", even if others are present)
--details             Include additional detailed results in output (see below)
--min_coverage        Minimum coverage required for alignment (default: 50)
--min_identity        Minimum percent identity required for alignment (default: 90)
--top                 Threshold used to retain overlapping alignments within --top% score of the max score (default: 50)
--fdr_method          Method used for FDR correction (default: fdr_bh)
--alpha               Alpha value used for FDR correction (default: 0.2)
--blast               Align with BLAST+ instead of DIAMOND
--no_associations     Exclude all analysis of CAG association metrics
--min_cag_size        Only include details for CAGs containing at least this number of genes (default: 5)
--min_cag_prop        Only include details for CAGs which align with this proportion of genes to a given genome (default: 0.25)
--window_size         Size of rolling window (for /genomes/map/<feature>/<genome_id>) (default: 5 [adjacent genes])

Output HDF:
The output from this pipeline is an HDF file which contains all of the data from the
input HDF, as well as the additional tables,

* /genomes/manifest (Table with the description of all genomes used for alignment)
* /genomes/cags/containment (Table with the number of genes aligned for all CAGs against all genomes)
* /genomes/detail/<genome_id> (Table with the coordinates of all aligned genes for a given genome)
* /genomes/annotation/<genome_id> (Table with the annotations from optional GFF input)
* /genomes/summary/<feature> (Table with summary metrics for each genome based on gene-level association with a given parameter)
* /genomes/map/<feature>/<genome_id> (Table with rolling window average of parameter association across the genome)

    """.stripIndent()
}

// Show help message if the user specifies the --help flag at runtime
if (params.help || params.geneshot_hdf == null || params.geneshot_dmnd == null || params.db == null || params.db == null || params.output_hdf == null){
    // Invoke the function above which prints the help message
    helpMessage()

    if (params.geneshot_hdf == null){
        log.info"""
        Please provide --geneshot_hdf
        """.stripIndent()
    }
    if (params.geneshot_dmnd == null){
        log.info"""
        Please provide --geneshot_dmnd
        """.stripIndent()
    }
    if (params.db == null){
        log.info"""
        Please provide --db
        """.stripIndent()
    }
    if (params.output_hdf == null){
        log.info"""
        Please provide --output_hdf
        """.stripIndent()
    }

    // Exit out and do not run anything else
    exit 1
}

// Make sure the input database file(s) exist
db_ch = Channel.from(
    params.db.split(",")
).map { 
    fp -> file(fp) 
}

// Point to the files with the GeneShot results
geneshot_hdf = file(params.geneshot_hdf)
if ( geneshot_hdf.isEmpty() ) {
    log.info"""
    Cannot find file at ${params.geneshot_hdf}
    """
    exit 1
}
geneshot_dmnd = file(params.geneshot_dmnd)
if ( geneshot_dmnd.isEmpty() ) {
    log.info"""
    Cannot find file at ${params.geneshot_dmnd}
    """
    exit 1
}

// Unpack the database
process unpackDatabase {
    tag "Extract all files from database tarball"
    container "ubuntu:20.04"
    label "io_limited"
    errorStrategy 'retry'

    input:
        file db from db_ch
    
    output:
        file "${db}.manifest.csv" into manifest_ch
        file "*tar" into database_tar_list
        file "genome_annotations.hdf5" into genome_annotations_hdf5

"""
#!/bin/bash 

set -e

ls -lahtr

tar xvf ${db}

mv database_manifest.csv ${db}.manifest.csv

# Make a dummy file for the genome annotations
if [[ ! -s genome_annotations.hdf5 ]]; then
    touch genome_annotations.hdf5
fi

echo "Done"

"""
}

// If multiple databases were provided, join the manifests
// Make sure that there are no overlapping IDs
process validateManifest {
    
    container "${container__pandas}"
    label 'io_limited'
    errorStrategy "retry"

    input:
        file manifest_csv_list from manifest_ch.toSortedList()
    
    output:
        file "database_manifest.csv" into valid_manifest_ch
    
"""
#!/usr/bin/env python3

import pandas as pd
import os

# Parse the list of files
manifest_csv_list = "${manifest_csv_list}".split(" ")

# Make sure all files are present
for fp in manifest_csv_list:
    assert os.path.exists(fp), "Could not find %s" % fp

# Read in and join the files
manifest_df = pd.concat([
    pd.read_csv(fp)
    for fp in manifest_csv_list
], sort=True)

# Make sure that none of the IDs are shared
vc = manifest_df["id"].value_counts()
assert vc.max() == 1, ("Found duplicated IDs across these databases", vc.head())

# Write out to a new file
manifest_df.to_csv("database_manifest.csv", index=None)

"""
}


// If using BLAST+, format the database
if (params.blast) {
    // Convert the gene catalog from DMND to FASTA
    process extractFASTA {
        container "${container_diamond}"
        label "io_limited"

        input:
            file geneshot_dmnd
        
        output:
            file "ref.fasta.gz" into ref_fasta


        """#!/bin/bash

        set -Eeuxo pipefail

        diamond getseq --db ${geneshot_dmnd} --out ref.fasta

        gzip ref.fasta
        """
    }

    // Format the BLAST database
    process makeBLASTdb {
        container "${container__blast}"
        label "mem_medium"

        input:
            file ref_fasta
        
        output:
            file "blastDB*" into blastDB


        """#!/bin/bash

        set -Eeuxo pipefail

        echo "Decompressing gene catalog FASTA"
        gunzip -c ${ref_fasta} > ref.fasta

        echo "Head of gene catalog FASTA"
        head ref.fasta

        echo "Building database"
        makeblastdb -in ref.fasta -dbtype prot -out blastDB

        echo "Done"
        """
    }

    // Align the genomes against the database with BLAST
    process alignGenomesBLAST {
        tag "Annotate reference genomes by alignment"
        container "${container__blast}"
        label "mem_medium"
        errorStrategy 'retry'

        input:
            file database_chunk_tar from database_tar_list.flatten()
            file "*" from blastDB

        output:
            tuple file("${database_chunk_tar.name.replaceAll(/.tar/, ".aln.gz")}"), file("${database_chunk_tar.name.replaceAll(/.tar/, ".csv.gz")}") into raw_alignment_ch

    """
    #!/bin/bash

    set -Eeuxo pipefail

    ls -lahtr

    tar xvf ${database_chunk_tar}

    blastx \
        -query <(gunzip -c ${database_chunk_tar.name.replaceAll(/.tar/, ".fasta.gz")}) \
        -db blastDB \
        -query_gencode 11 \
        -outfmt "6 qseqid sseqid pident length qstart qend qlen sstart send slen" \
        -num_threads ${task.cpus} \
        -max_target_seqs 10000000 \
        -evalue 0.001 \
        | gzip -c > ${database_chunk_tar.name.replaceAll(/.tar/, ".aln.gz")}

    """
    }

    // Filter alignments by percent identity and coverage of the gene catalog entry
    process filterBLASThits {
        container "${container__pandas}"
        label "mem_medium"
        errorStrategy 'retry'

        input:
            tuple file(aln_tsv_gz), file(header_csv_gz) from raw_alignment_ch

        output:
            tuple file("${aln_tsv_gz}.filtered.tsv.gz"), file("${header_csv_gz}") into alignments_ch

"""#!/usr/bin/env python3

import pandas as pd

# Read in the table of hits
df = pd.read_csv("${aln_tsv_gz}", sep="\\t", header=None)
print("Read in %d alignments" % df.shape[0])

# Filter by percent identity
df = df.loc[
    df[2] >= ${params.min_identity}
]
print("Filtered down to %d alignments with identity >= ${params.min_identity}" % df.shape[0])

# Filter by coverage
df = df.assign(
    coverage = 100 * df[3] / df[9]
).query(
    "coverage >= ${params.min_coverage}"
).drop(
    columns = "coverage"
)
print("Filtered down to %d alignments with coverage >= ${params.min_coverage}" % df.shape[0])

# Write out
df.to_csv("${aln_tsv_gz}.filtered.tsv.gz", sep="\\t", index=None, header=None)

"""
    }

} else {

    // Align the genomes against the database with DIAMOND
    process alignGenomes {
        tag "Annotate reference genomes by alignment"
        container "${container_diamond}"
        label "mem_veryhigh"
        errorStrategy 'retry'

        input:
            file database_chunk_tar from database_tar_list.flatten()
            file geneshot_dmnd

        output:
            tuple file("${database_chunk_tar.name.replaceAll(/.tar/, ".aln.gz")}"), file("${database_chunk_tar.name.replaceAll(/.tar/, ".csv.gz")}") into alignments_ch

    """
    #!/bin/bash

    set -Eeuxo pipefail

    ls -lahtr

    tar xvf ${database_chunk_tar}

    diamond \
        blastx \
        --db ${geneshot_dmnd} \
        --query ${database_chunk_tar.name.replaceAll(/.tar/, ".fasta.gz")} \
        --out ${database_chunk_tar.name.replaceAll(/.tar/, ".aln.gz")} \
        --outfmt 6 qseqid sseqid pident length qstart qend qlen sstart send slen \
        --id ${params.min_identity} \
        --subject-cover ${params.min_coverage} \
        --top ${params.top} \
        --compress 1 \
        --unal 0 \
        --sensitive \
        --query-gencode 11 \
        --range-culling \
        -F 1 \
        --block-size ${task.memory.toMega() / (1024 * 6 * task.attempt)} \
        --threads ${task.cpus} \

    """
    }

}


// Calculate the containment of each CAG in each genome
process filterAlignments {
    
    container "ubuntu:20.04"
    label 'io_limited'
    errorStrategy 'retry'

    input:
        tuple file(aln_tsv_gz), file(header_csv_gz) from alignments_ch

    output:
        tuple file("${aln_tsv_gz}"), file("${header_csv_gz}") optional true into alignments_ch_1, alignments_ch_2

"""
#!/bin/bash

set -e

if (( \$( cat ${aln_tsv_gz} | wc -l ) > 0 )); then

    echo "Number of alignments: \$( cat ${aln_tsv_gz} | wc -l )"

else

    echo "No alignments found, filtering out"

    rm ${aln_tsv_gz} ${header_csv_gz}

fi

"""

}
// Calculate the containment of each CAG in each genome
process calculateContainment {
    tag "Overlap between CAGs and genomes"
    container "${container__pandas}"
    label 'mem_veryhigh'
    errorStrategy 'retry'

    input:
        tuple file(aln_tsv_gz), file(header_csv_gz) from alignments_ch_2
        file geneshot_hdf

    output:
        file "genome_containment_shard.*.csv.gz" optional true into containment_shard_csv_gz
        file "genome_containment_shard.*.hdf5" optional true into genome_alignment_shards

    script:
        template "calculateContainment.py"

}

// Join all of the results of the containment calculation
containment_shard_csv_gz_list = containment_shard_csv_gz.toSortedList()

if (params.no_associations == false){

    // Check to make sure that the input HDF has the required entries
    process parseAssociations {
        tag "Extract gene association data for the study"
        container "${container__pandas}"
        label 'mem_veryhigh'
        errorStrategy "retry"

        input:
            file geneshot_hdf
        
        output:
            file "gene_associations.*.csv.gz" into gene_association_csv_ch
        
        script:
            template "parseAssociations.py"
    }


    // Format the results for each shard
    process formatResults {
        tag "Use alignment information to summarize results"
        container "${container__pandas}"
        label 'mem_medium'
        errorStrategy "retry"

        input:
            tuple file(aln_tsv_gz), file(header_csv_gz) from alignments_ch_1
            each file(gene_association_csv) from gene_association_csv_ch.flatten()
        
        output:
            file "genome_association_shard.*.hdf5" optional true into association_shard_hdf
        
        script:
            template "formatResults.py"
    }

    // Join together the association data in batches of 100
    process joinAssociationsRoundOne {
        container "${container__pandas}"
        label 'mem_medium'
        errorStrategy "retry"

        input:
            file "association_shard.*.hdf5" from association_shard_hdf.toSortedList().flatten().collate(100)
        
        output:
            file "joined_associations.hdf5" into joined_association_shard_hdf
        
        script:
            template "joinAssociations.py"
    }

    // Join together all of those batches into a single file
    process joinAssociationsRoundTwo {
        container "${container__pandas}"
        label 'mem_medium'
        errorStrategy "retry"

        input:
            file "association_shard.*.hdf5" from joined_association_shard_hdf.toSortedList()
        
        output:
            file "joined_associations.hdf5" into joined_associations_hdf
        
        script:
            template "joinAssociations.py"
    }

    // Collect results and combine across all shards
    process combineResults {
        tag "Make a single output HDF"
        container "${container__pandas}"
        label 'mem_veryhigh'
        errorStrategy "retry"

        input:
            file "containment_shard.*.csv.gz" from containment_shard_csv_gz_list
            file "associations.hdf5" from joined_associations_hdf
            file geneshot_hdf
            file manifest_csv from valid_manifest_ch
            file "genome_alignments.*.hdf5" from genome_alignment_shards.toSortedList()
            file "genome_annotations.hdf5" from genome_annotations_hdf5
        
        output:
            file "${params.output_hdf}" into final_hdf

        script:
            template "combineResults.py"

    }
} else {
    // Collect results and combine across all shards
    process combineResultsNoAssoc {
        tag "Make a single output HDF"
        container "${container__pandas}"
        label 'mem_veryhigh'
        errorStrategy "retry"

        input:
            file containment_shard_csv_gz_list
            file geneshot_hdf
            file manifest_csv from valid_manifest_ch
            file "genome_alignments.*.hdf5" from genome_alignment_shards.toSortedList()
            file "genome_annotations.hdf5" from genome_annotations_hdf5
        
        output:
            file "${params.output_hdf}" into final_hdf
        
        script:
            template "combineResultsNoAssoc.py"
    }
}



// Repack an HDF5 file
process repackHDF {

    container "${container__pandas}"
    tag "Compress HDF store"
    label "mem_medium"
    errorStrategy "retry"
    publishDir "${params.output_folder}"
    
    input:
    file final_hdf
        
    output:
    file "${final_hdf}"

    """
#!/bin/bash

set -Eeuxo pipefail

[ -s ${final_hdf} ]

h5repack -f GZIP=5 ${final_hdf} TEMP && mv TEMP ${final_hdf}
    """
}