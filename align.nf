#!/usr/bin/env nextflow

// Set default parameters
params.help = false
params.geneshot_hdf = null
params.geneshot_fasta = null
params.db_dmnd = null
params.db_hdf = null
params.output_hdf = null
params.min_coverage = 50
params.min_identity = 80
params.batchsize = 1000
params.fdr_method = "fdr_bh"
params.alpha = 0.2

// Function which prints help message text
def helpMessage() {
    log.info"""
    Usage:

    nextflow run FredHutch/AMGMA <ARGUMENTS>
    
    Required Arguments:
      --geneshot_hdf        Results HDF file output by GeneShot, containing CAG information
      --geneshot_fasta      Gzip-compressed FASTA with the gene catalog generated by GeneShot
      --db_dmnd             Microbial genome database, DMND format
      --db_hdf              Microbial genome database, HDF format
      --output_hdf          Path to output HDF file

    Optional Arguments:
      --min_coverage        Minimum coverage required for alignment (default: 50)
      --min_identity        Minimum percent identity required for alignment (default: 80)
      --batchsize           Number of genomes to process in a given batch
      --fdr_method          Method used for FDR correction (default: fdr_bh)
      --alpha               Alpha value used for FDR correction (default: 0.2)

    Database files:
    The DMND and HDF files used as inputs for this script are produced by the build_db.nf
    function provided as part of this project.

    Input HDF:
    The information on CAGs used as an input for this analysis is formatted as the output
    from the GeneShot pipeline (more information at github.com/Golob-Minot/GeneShot).

    Input FASTA:
    Another output from the GeneShot pipeline is the set of genes which were identified,
    in gzip-compressed FASTA format, specified as input here with --geneshot_fasta

    Output HDF:
    The output from this pipeline is an HDF file which contains all of the data from the
    input HDF, as well as the additional tables, 

      * /genomes/<parameter>/summary
      * /genomes/<parameter>/<genome_id>

      for each <parameter> tested in the input, and for each <genome_id> in the database

    """.stripIndent()
}

// Show help message if the user specifies the --help flag at runtime
if (params.help || params.geneshot_hdf == null || params.geneshot_fasta == null || params.db_dmnd == null || params.db_hdf == null || params.output_hdf == null){
    // Invoke the function above which prints the help message
    helpMessage()

    if (params.geneshot_hdf == null){
        log.info"""
        Please provide --geneshot_hdf
        """.stripIndent()
    }
    if (params.geneshot_fasta == null){
        log.info"""
        Please provide --geneshot_fasta
        """.stripIndent()
    }
    if (params.db_dmnd == null){
        log.info"""
        Please provide --db_dmnd
        """.stripIndent()
    }
    if (params.db_hdf == null){
        log.info"""
        Please provide --db_hdf
        """.stripIndent()
    }
    if (params.output_hdf == null){
        log.info"""
        Please provide --output_hdf
        """.stripIndent()
    }

    // Exit out and do not run anything else
    exit 0
}


// Check to make sure that the input HDF has the required entries
process parseAssociations {
    tag "Extract gene association data for the study"
    container "quay.io/fhcrc-microbiome/python-pandas:latest"
    label 'io_limited'
    errorStrategy "retry"

    input:
        file geneshot_hdf from file(params.geneshot_hdf)
    
    output:
        file "gene_associations.*.csv.gz" into gene_association_csv_ch
    
"""
#!/usr/bin/env python3

import pandas as pd
from statsmodels.stats.multitest import multipletests

store_fp = "${geneshot_hdf}"

###################
# READ INPUT DATA #
###################

with pd.HDFStore(store_fp, "r") as store:

    for k in ["/stats/cag/corncob", "/annot/gene/all"]:
        assert k in store, "Could not find %s in %s" % (k, store_fp)

    corncob_df = pd.read_hdf(store, "/stats/cag/corncob")

    annot_df = pd.read_hdf(store, "/annot/gene/all")


#######################
# FORMAT CORNCOB DATA #
#######################

# Filter down to the mu estimates
corncob_df = corncob_df.loc[
    corncob_df["parameter"].apply(lambda s: s.startswith("mu."))
]
print("Corncob results have %d rows for mu" % (corncob_df.shape[0]))
assert corncob_df.shape[0] > 0

# Remove the intercept values
corncob_df = corncob_df.loc[
    corncob_df["parameter"] != "mu.(Intercept)"
]
print("Corncob results have %d non-intercept rows for mu" % (corncob_df.shape[0]))
assert corncob_df.shape[0] > 0

# Remove the "mu." from the parameter
corncob_df["parameter"] = corncob_df["parameter"].apply(lambda s: s[3:])

# Reformat the corncob results as a dict
corncob_dict = dict([
    (parameter, parameter_df.pivot_table(index="CAG", columns="type", values="value"))
    for parameter, parameter_df in corncob_df.groupby("parameter")
])

# Add in the FDR threshold
for parameter in corncob_dict:
    corncob_dict[
        parameter
    ][
        "${params.fdr_method}"
    ] = multipletests(
        corncob_dict[parameter]["p_value"].fillna(1),
        ${params.alpha},
        "${params.fdr_method}"
    )[1]

print("Processing %d parameters: %s" % (
    corncob_df["parameter"].unique().shape[0],
    ", ".join(corncob_df["parameter"].unique())
))

#########################
# FORMAT CAG MEMBERSHIP #
#########################

# Make sure the CAG gene membership table has values
print("CAG membership table has %d rows" % (annot_df.shape[0]))
assert annot_df.shape[0] > 0

# Make sure the expected columns exist
assert "CAG" in annot_df.columns.values and "gene" in annot_df.columns.values

# Make sure that every CAG in the corncob results has an entry in the gene membership table
cag_set = set(annot_df["CAG"].tolist())
for cag_id in corncob_df["CAG"].unique():
    assert cag_id in cag_set, "Could not find genes for CAG %s" % cag_id

######################
# FORMAT OUTPUT DATA #
######################

# For each parameter, write out a table with the association for each gene
for parameter, cag_assoc in corncob_dict.items():
    print("Processing %s" % parameter)
    
    # Make a gene-level association table
    gene_assoc = annot_df.copy()

    # Add the CAG-level associations
    for k in cag_assoc.columns.values:
        gene_assoc[k] = gene_assoc["CAG"].apply(
            cag_assoc[k].get
        )
    # Write out to CSV
    gene_assoc.to_csv(
        "gene_associations.%s.csv.gz" % parameter,
        index = None,
        compression = "gzip",
        sep = ","
    )


print("All done!")
"""
}


// Align genes from the database against genes from the CAGs with DIAMOND
process alignGenes {
    tag "Align genes against reference database"
    container "quay.io/fhcrc-microbiome/famli@sha256:25c34c73964f06653234dd7804c3cf5d9cf520bc063723e856dae8b16ba74b0c"
    label 'mem_veryhigh'
    errorStrategy 'retry'
    
    input:
    file geneshot_fasta from file(params.geneshot_fasta)
    file db_dmnd from file(params.db_dmnd)

    output:
    file "genes.DB.aln.gz" into aln_tsv

    """
#!/bin/bash

set -e

diamond \
    blastp \
    --query "${geneshot_fasta}" \
    --db "${db_dmnd}" \
    --threads ${task.cpus} \
    --out genes.DB.aln.gz \
    --threads ${task.cpus} \
    --outfmt 6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen slen \
    --query-cover ${params.min_coverage} \
    --id ${params.min_identity} \
    --block-size ${task.memory.toMega() / (1024 * 6)} \
    --compress 1 \
    --unal 0

    """

}


// Split up the genomes into shards for parallel processing
process splitGenomes {
    tag "Group genomes for parallel processing"
    container "quay.io/fhcrc-microbiome/python-pandas:latest"
    label 'mem_medium'
    errorStrategy "retry"

    input:
        file db_hdf from file(params.db_hdf)

    output:
        file "genome_list.*.txt.gz" into genome_list_ch

"""
#!/usr/bin/env python3

import gzip
import pandas as pd

manifest = pd.read_hdf("${db_hdf}", "/manifest")

for ix, genome_list in enumerate([
    manifest["id"].values[ix: (ix + ${params.batchsize})]
    for ix in range(0, manifest.shape[0], ${params.batchsize})
]):
    with gzip.open("genome_list.%d.txt.gz" % ix, "wt") as fo:
        fo.write("\\n".join(genome_list))

"""

}


// Format the results
process formatResults {
    tag "Use alignment information to summarize results"
    container "quay.io/fhcrc-microbiome/python-pandas:latest"
    label 'mem_medium'
    // errorStrategy "retry"

    input:
        file genome_list from genome_list_ch.flatten()
        each file(gene_association_csv) from gene_association_csv_ch
        file aln_tsv
        file db_hdf from file(params.db_hdf)
    
    output:
        file "genome_analysis_shard.hdf5"
    
"""
#!/usr/bin/env python3

import gzip
import pandas as pd

##########################
# READ GENE ASSOCIATIONS #
##########################

gene_assoc_df = pd.read_csv(
    "${gene_association_csv}",
    sep = ",",
    compression = "gzip"
).set_index(
    "gene"
)


########################
# PARSE PARAMETER NAME #
########################

# Parse the parameter name from the gene association CSV file name
assert "${gene_association_csv}".startswith("gene_associations.")
assert "${gene_association_csv}".endswith(".csv.gz")
parameter_name = "${gene_association_csv}".replace(
    "gene_associations.", ""
).replace(
    ".csv.gz", ""
)

print("Analyzing parameter: %s" % (parameter_name))


##########################
# FORMAT GENE ALIGNMENTS #
##########################

# Read in the alignments of reference genome genes against the gene catalog genes
# Reformat to a dict linking every reference genome gene to a single entry in the gene catalog
aln_df = pd.read_csv(
    "${aln_tsv}", 
    sep="\\t", 
    header=None
).reindex(
    columns=[0, 1, 11]
).rename(columns=dict([
    (0, "catalog_gene"),
    (1, "genome_gene"),
    (11, "score")
])).sort_values(
    by = "score",
    ascending = False
).drop(
    columns = "score"
).groupby(
    "genome_gene"
).head(
    1
).set_index(
    "genome_gene"
)

print("Read in %d gene alignments" % aln_df.shape[0])

##################
# SUBSET GENOMES #
##################

# Read in the group of genomes to process in this shard
genome_list = [
    line.rstrip("\\n")
    for line in gzip.open("${genome_list}", "rt")
]
print("Processing %d genomes" % len(genome_list))

# Open a connection to the HDF store with genome alignment information
genome_store = pd.HDFStore("${db_hdf}", "r")

# Open a connection to the HDF store used for all output information
output_store = pd.HDFStore("genome_analysis_shard.hdf5", "w")

# Function to process a single genome
def process_genome(genome_id):

    # Read in the alignment to that genome
    genome_aln_df = pd.read_hdf(
        genome_store, 
        "/genomes/%s" % genome_id
    )

    # Add in the 'catalog' gene label
    genome_aln_df["catalog_gene"] = genome_aln_df["gene_id"].apply(
        aln_df["catalog_gene"].get
    )

    # Add in the gene annotations
    for k in gene_assoc_df.columns.values:

        # To annotate the genome, figure out which of the catalog genes
        # each of the genes in the genome is most similar to, and then
        # fill in the value of the CAG which that catalog gene is a part of

        genome_aln_df[k] = genome_aln_df[
            "gene_id"
        ].apply(
            aln_df["catalog_gene"].get
        ).apply(
            gene_assoc_df[k].get
        )

    # Write out the full table
    key = "/genomes/%s/%s" % (parameter_name, genome_id)
    print("Writing out to %s" % key)
    gene_assoc_df.to_hdf(
        output_store,
        key,
        format = "fixed",
        complevel = 5
    )

    # Get the table which passes the FDR filter
    gene_assoc_df_fdr = gene_assoc_df.loc[
        gene_assoc_df["${params.fdr_method}"] <= ${params.alpha}
    ]

    print("%d / %d genes pass the CAG-level FDR threshold" % 
        (gene_assoc_df_fdr.shape[0], gene_assoc_df.shape[0]))

    # Return the summary metrics
    return dict([
        ("genome_id", genome_id),
        ("parameter", parameter_name),
        ("total_genes", gene_assoc_df.shape[0]),
        ("n_pass_fdr", gene_assoc_df_fdr.shape[0]),
        ("prop_pass_fdr", gene_assoc_df_fdr.shape[0] / float(gene_assoc_df.shape[0])),
        ("mean_est_coef", gene_assoc_df_fdr["estimate"].mean())
    ])

# Iterate over every genome and process it, saving the summary to the HDF
pd.DataFrame([
    result
    for genome_id in genome_list
    for result in process_genome(genome_id)
]).to_hdf(
    output_store,
    "/genomes/%s" % parameter_name,
    format = "fixed"
)


######################
# CLOSE OUTPUT FILES #
######################

genome_store.close()
output_store.close()

"""
}